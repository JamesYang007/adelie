\section{Optimizer Algorithm}\label{sec:optimizer}

In this section, we discuss the algorithm to solve the optimization problem~(\ref{eq:glp:objective}).
Let $X \in \R^{n\times p}$, $y\in \R^n$ be the feature matrix and response vector, respectively.
Let $G$ be the number of groups and $p_i$ be the size of group $i$ for $1\leq i\leq G$.
Further let $X = \br{X_1 \, X_2 \, \ldots \, X_G}$ where $X_i \in \R^{n \times p_i}$ is the feature matrix
corresponding to group $i$ features only.

In general, if the loss function $f$ is of the form $f(\beta) \equiv f(y, X\beta)$,
the optimization problem~(\ref{eq:glp:objective}) can be reparametrized
in terms of $\tilde{\beta} \in \R^p$ where $\tilde{\beta}_i := Q_i^\top \beta_i$
where $Q_i \in \R^{p_i \times p_i}$ is the orthogonal matrix from the eigen-decomposition of 
$X_i^\top X_i = Q_i D_i Q_i^\top$ for each $i=1,\ldots, G$.
Concretely, it suffices to solve
\begin{align}
    \minimize_{\tilde{\beta} \in \R^p}
    f(y, \tilde{X} \tilde{\beta})
    + \lambda \sum\limits_{j=1}^G \norm{\tilde{\beta}_j}_2
    \label{eq:oa:new-objective}
\end{align}
where $\tilde{X} = \br{X_1 Q_1 \, X_2 Q_2 \, \ldots \, X_G Q_G}$.
Note that this was only possible because the group lasso penalty is invariant under
the transformation from $\beta \mapsto \tilde{\beta}$.
Then, given a solution $\tilde{\beta}^*$ to the new problem~(\ref{eq:oa:new-objective}),
we have the solution to the original problem $\beta^*$ given by
$\beta^*_i := Q_i \tilde{\beta}_i^*$.
Hence, without loss of generality, we assume that 
our feature matrix $X$ is of the form $\tilde{X}$.
In particular, we assume that $X_i^\top X_i = D_i$ is diagonal for all $1\leq i\leq G$.
We drop all tilde for notational simplicity.

Computationally, it may seem daunting to 
construct $\tilde{X}$ from $X$ before solving~(\ref{eq:oa:new-objective}).
However, the time taken for this pre-processing is relatively cheap compared to the full optimizing algorithm.
Given the data matrix $X$, consider a block $X_i \in \R^{n \times p_i}$.
we first perform the Singular Value Decomposition (SVD) such that
$X_i = U_iD_iV_i^T$ where $U_i \in \R^{n \times m_i}$, $D_i \in \R^{m_i \times p_i}$ and $V_i \in \R^{p_i \times p_i}$,
where $m_i = \min(n,p_i)$.
Note that $V_i$ is precisely the $Q_i$ from the preceding discussion.
Hence, $\tilde{X}_i \equiv U_i D_i$.
If $n \geq p_i$, then $\tilde{X}_i$ can be computed in $O(n p_i)$ time, noting that $D_i$ is diagonal.
Otherwise if $n < p_i$, then $D_i = [D_{i0} \; \textbf{0}_{n\times (p-n)}]$ where $D_{i0} \in \R^{n \times n}$ is the diagonal matrix.
Then, $\tilde{X}_i = [U_i D_{i0} \; \textbf{0}_{n \times (p-n)}]$ so that the only product to be computed takes $O(n^2)$ time.
Hence, the total complexity of the procedure is
\begin{align*}
    O\pr{
        \sum\limits_{i=1}^G
        \pr{\underbrace{\min(n,p_i) n p_i}_{\svd} + \underbrace{\min(n, p_i) n}_{\tilde{X}_i}}
    }
    =
    O\pr{
        n
        \sum\limits_{i=1}^G
        \min(n,p_i) p_i
    }
\end{align*}
Moreover, since this algorithm is parallelizable across groups,
we can reduce the time by the number of parallel threads.
This pre-computation complexity is smaller than the proposed optimizer algorithm\todojames{compute?}
and is a one-time computation.
Thus, it is worth performing this pre-computation for the large speed-ups in the optimizing algorithm.

In comparison to other proposed fitting procedures,
we believe that the extra cost to compute $\tilde{X}_i$, $D_i$, and $\tilde{X}_i^\top \tilde{X}_j$
is comparable to the cost of the other procedures.
For example, \citet{yuan:2006,meier:2008} rely on the fact that
each group feature matrix $X_i$ is full-rank and therefore admits a QR decomposition with an invertible $R$.
Under this assumption, they solve~(\ref{eq:oa:new-objective}) with $\tilde{X} = \br{Q_1 \, Q_2 \, \ldots \, Q_G}$
and transform the solution back to the original scale by applying $R_i^{-1}$ to the $i$th group of coefficients.
Note that this also applies a one-time decomposition of the group feature matrices.
The benefit of this approach is that the algorithm is aided by a simple closed-form expression
as part of the blockwise coordinate descent algorithm.
However, there are two major downsides to this approach.
The first is that, while the aforementioned authors primarily focused on the application of group lasso 
on multi-level categorical data that easily satisfy the full-rank assumption,
this assumption may easily be violated in other applications.
For example,~\todojames{mention GWAS; small n, bigger group size}.
The second is that this method only gives an approximation to the original problem.
That is, given the solution to~(\ref{eq:oa:new-objective}), $\tilde{\beta}^*$,
the candidate $\beta^\star$ defined by $\beta^\star_i := R_i^{-1} \tilde{\beta^\star}_i$ 
for $i=1,\ldots, G$ is not necessarily a solution to~(\ref{eq:glp:objective})
with the original data.
Indeed, for this approximation to be exact, 
rather than solving~(\ref{eq:oa:new-objective}),
one should solve
\begin{align*}
    \minimize_{\beta \in \R^p}
    f(y, \tilde{X} \tilde{\beta})
    + \lambda \sum\limits_{j=1}^G \norm{R_i^{-1} \tilde{\beta}_j}_2
\end{align*}
which introduces a Tikhonov regularization.
Although this remains a convex optimization problem and can be solved,
it does not simplify any computation since it does not enjoy the same 
closed-form expressions as the approximation method does.

Similar to other approaches~\citep{yuan:2006,meier:2008,tseng:2001,sparsegl:2022},
we use blockwise coordinate descent for its excellent speed and simplicity.
However, we take a completely different approach to fitting each block coordinate update.
The essence of implementing a highly performant optimizer lies in 
solving each block update~(\ref{eq:bcd:block-update}) as fast as possible.
Aside from the orthonormalization method described above as in~\citet{yuan:2006,meier:2008},
many works in literature that describe group lasso fitting procedures
use the general descent method called Proximal Gradient Descent (PGD), 
or Iterative Soft-Thresholding Algorithm (ISTA) in this context,
to solve~(\ref{eq:bcd:block-update})~\citep{sparsegl:2022,beck:2009,klosa:2020,wright:2009,loris:2009,sls:2016,odonoghue:2015}.
A Nesterov acceleration can be applied on ISTA to get Fast-ISTA (FISTA)~\citep{beck:2009}.
Further,~\citet{odonoghue:2015} showed that mild improvements can be made with an adaptive restarting strategy.
\Cref{ssec:bcd} describes the blockwise coordinate descent algorithm.
\Cref{ssec:pgd} describes the PGD (ISTA) procedure and its variations.
\Cref{ssec:newton,ssec:newton-abs} describe our proposed method to solve~(\ref{eq:bcd:block-update}).

\input{algorithm/bcd}
\input{algorithm/pgd}
\input{algorithm/newton}
\input{algorithm/newton-abs}