\section{Optimizer Algorithm}

In this section, we discuss the algorithm to solve the optimization problem~(\ref{eq:glp:objective}).
Let $X \in \R^{n\times p}$, $y\in \R^n$ be the feature matrix and response vector, respectively.
Let $G$ be the number of groups and $p_i$ be the size of group $i$ for $1\leq i\leq G$.
Further let $X = \br{X_1 \, X_2 \, \ldots \, X_G}$ where $X_i \in \R^{n \times p_i}$ is the feature matrix
corresponding to group $i$ features only.

In general, if the loss function $f$ is of the form $f(\beta) \equiv f(y, X\beta)$,
the optimization problem~(\ref{eq:glp:objective}) can be reparametrized
in terms of $\tilde{\beta} \in \R^p$ where $\tilde{\beta}_i := Q_i^\top \beta_i$
where $Q_i \in \R^{p_i \times p_i}$ is the orthogonal matrix from the eigen-decomposition of 
$X_i^\top X_i = Q_i D_i Q_i^\top$ for each $i=1,\ldots, G$.
Concretely, it suffices to solve
\begin{align}
    \minimize_{\tilde{\beta} \in \R^p}
    f(y, \tilde{X} \tilde{\beta})
    + \lambda \sum\limits_{j=1}^G \norm{\tilde{\beta}_j}_2
    \label{eq:oa:new-objective}
\end{align}
where $\tilde{X} = \br{X_1 Q_1 \, X_2 Q_2 \, \ldots \, X_G Q_G}$.
Note that this was only possible because the group lasso penalty is invariant under
the transformation from $\beta \to \tilde{\beta}$.
Then, given a solution $\tilde{\beta}^*$ to the new problem~(\ref{eq:oa:new-objective}),
we have the solution to the original problem $\beta^*$ given by
$\beta^*_i := Q_i^\top \tilde{\beta}_i^*$.
Hence, without loss of generality, we assume that 
our feature matrix $X$ is of the form $\tilde{X}$.
In particular, we assume that $X_i^\top X_i = D_i$ is diagonal for all $1\leq i\leq G$.
We drop all tilde for notational simplicity.

Computationally, it may seem like 
we must construct $\tilde{X}$ from $X$ before solving~(\ref{eq:oa:new-objective}).
However, we will shortly see that the fitting algorithm can be amended to 
dynamically compute the necessary components of $\tilde{X}$.\todojames{Reference to later section?}
In particular, we only need access to $\tilde{X}_i$, $D_i$,
and $\tilde{X}_i^\top \tilde{X}_j$ for $i$ in the strong set and $j$ in the active set.
In comparison to other proposed fitting procedures,
we believe that the extra cost to compute $\tilde{X}_i$ and $\tilde{X}_i^\top \tilde{X}_j$
is comparable to the cost of the other procedures.
For example, \citet*{yuan:2006,meier:2008} rely on the fact that
each group feature matrix $X_i$ is full-rank and therefore admits a QR decomposition with an invertible $R$.
Under this assumption, they solve~(\ref{eq:oa:new-objective}) with $\tilde{X} = \br{Q_1 \, Q_2 \, \ldots \, Q_G}$
and transform the solution back to the original scale by applying $R_i^{-1}$ to the $i$th group of coefficients.
Note that these other approaches also apply a one-time decomposition of the group feature matrices.
The benefit of this approach is that the algorithm is aided by a simple closed-form expression
as part of the blockwise coordinate descent algorithm.
However, there are two major downsides to this approach.
The first is that, while the aforementioned authors primarily focused on the application of group lasso 
on categorical data such as one-hot encoded data that easily satisfy the full-rank assumption,
this assumption may easily be violated in other applications.
For example,~\todojames{mention GWAS; small n, bigger group size}.
The second is that this method does not simplify computations 
when we add a ridge penalty as well (i.e. elastic net), 
since in general, it introduces a Tikhonov regularization in the transformed problem.

Similar to other approaches~\citep{yuan:2006,meier:2008,tseng:2001,sparsegl:2022},
we use blockwise coordinate descent for its excellent speed and simplicity.
However, we take a completely different approach to fitting each block update.
The essence of implementing a highly performant optimizer lies in 
solving each block update~(\ref{eq:bcd:block-update}) as fast as possible.
Aside from the orthonormalization method described above as in~\citet*{yuan:2006,meier:2008},
many works in literature that describe group lasso fitting procedures
use the general descent method called Proximal Gradient Descent (PGD), 
or Iterative Soft-Thresholding Algorithm (ISTA) in this context,
to solve~(\ref{eq:bcd:block-update})~\citep{sparsegl:2022,beck:2009,klosa:2020,wright:2009,loris:2009,sls:2016,odonoghue:2015}.
A Nesterov acceleration can be applied on ISTA to get Fast-ISTA (FISTA)~\citep{beck:2009}.
Further,~\citet{odonoghue:2015} showed that mild improvements can be made with an adaptive restarting strategy.
\Cref{ssec:bcd} describes the blockwise coordinate descent algorithm.
\Cref{ssec:pgd} describes the PGD (ISTA) procedure and its variations.
\Cref{ssec:newton,ssec:newton-abs} describe our proposed method to solve~(\ref{eq:bcd:block-update}).

\input{algorithm/bcd}
\input{algorithm/pgd}
\input{algorithm/newton}
\input{algorithm/newton-abs}