\appendix

\section{Newton Method Results}\label{appendix:newton}

\subsection{Properties of Root-Finding Function}\label{appendix:properties-varphi}

Let the function to root-find, $\varphi$, be as in~(\ref{eq:varphi-def}).
Upon differentiating $\varphi$,
\begin{align*}
    \frac{d\varphi(h)}{dh}
    &=
    -2 \sum\limits_{i=1}^p
    \frac{v_i^2 \Sigma_{ii}}{(\Sigma_{ii} h + \lambda)^3}
    \leq
    0
\end{align*}
where the inequality is strict if and only if not all $v_i^2 \Sigma_{ii} = 0$.
Note that if all $v_i^2 \Sigma_{ii} = 0$, then we must have that $0 = \norm{v}_2 \leq \lambda$.
This is because by assumption $v = D^\top u$ for some $u \in \R^p$
(see~(\ref{eq:bcd:block-update})), so that 
if $v_i^2 \Sigma_{ii} = 0$, either $v_i = 0$ or $\Sigma_{ii} = 0 \implies v_i = 0$.
Hence, without loss of generality, we may assume 
that, in particular, not all $\Sigma_{ii} = 0$ and
$\varphi'$ is strictly negative so that $\varphi$ is strictly decreasing.
Further, we have that
\begin{align*}
    \varphi(h)
    &=
    \sum\limits_{i: \Sigma_{ii} > 0}
    \frac{v_i^2}{(\Sigma_{ii} h + \lambda)^2} - 1
\end{align*}
Consequently, since $\varphi(0) > 0$ by hypothesis and
$\lim\limits_{h\to\infty} \varphi(h) = -1$,
there exists a (unique) root.
Further, it is easy to see that $\varphi$ is convex since it is a 
sum of convex functions.

\subsection{Convergence of Newton Method}\label{appendix:newton-convergence}

Since $\varphi$ is convex, this suggests solving~(\ref{eq:newton:norm-solution})
via a one-dimensional Newton's Method.
Specifically, with $h^{(0)}$ as the initial starting point, for $k\geq 1$,
\begin{align}
    h^{(k)} = h^{(k-1)} - \frac{\varphi(h^{(k-1)})}{\varphi'(h^{(k-1)})}
    \label{eq:newton:newton-step}
\end{align}
We claim that Newton's Method is guaranteed to converge for any initial point $h^{(0)}$.
Indeed, for every $k\geq 1$, by convexity of $\varphi$,
assuming $h^{(k)} \in [0,\infty)$,
\begin{align*}
    \varphi(h^{(k)})
    &\geq
    \varphi(h^{(k-1)})
    + \varphi'(h^{(k-1)}) (h^{(k)} - h^{(k-1)})
    =
    0
\end{align*}
Along with~(\ref{eq:newton:newton-step}),
this shows that $h^{(k)}$ is an increasing sequence for $k\geq 1$ 
and bounded above by $h^\star$, the root of $\varphi$, by monotonicity.
Hence, $h^{(k)}$ converges to some limit $h^{(\infty)}$. 
From~(\ref{eq:newton:newton-step}), taking limit as $k\to \infty$ and using that $\varphi'$ is non-zero,
\begin{align*}
    h^{(\infty)} = h^{(\infty)} - \frac{\varphi(h^{(\infty)})}{\varphi'(h^{(\infty)})}
    \implies
    \varphi(h^{(\infty)}) = 0
\end{align*}
which shows that $h^{(\infty)}$ is the root of $\varphi$.

\section{Newton-ABS Results}\label{appendix:newton-abs}

\subsection{Lower and Upper Bounds on the Root}\label{appendix:newton-abs:bounds}

In this section, we derive the lower and upper bounds on the root, $h_\star$, $h^\star \in [0,\infty)$,
as discussed in~\Cref{ssec:newton-abs}.
Specifically, we show that $\varphi(h_\star) \geq 0 \geq \varphi(h^\star)$,
which implies that the root lies in $[h_\star, h^\star]$,
since $\varphi$ is 
(without loss of generality)
strictly decreasing (\Cref{appendix:properties-varphi}).

We first begin with the lower bound $h_\star$.
Recall that we wish to relax the problem~(\ref{eq:newton-abs:lower-bound-problem}).
Let $a(h), b(h) \in \R^p$ be defined by $a_k(h) := \Sigma_{kk} h + \lambda$
and $b_k(h) := \frac{\abs{v_k}}{a_k(h)}$ for each $k=1,\ldots, p$.
Then, by Cauchy-Schwarz,
\begin{align*}
    \norm{v}_1
    &:=
    \sum\limits_{k} \abs{v_k}
    =
    \sum\limits_{k} a_k(h) b_k(h)
    \leq
    \norm{a(h)}_{2} \norm{b(h)}_2
\end{align*}
Hence, if $\norm{a(h)}_2 \leq \norm{v}_1$,
then $\varphi(h) \geq 0$.
We see that $\norm{a(h)}_2 \leq \norm{v}_1$ if and only if
\begin{align*}
    \sum\limits_{i=1}^p
    (\Sigma_{ii} h + \lambda)^2
    \leq
    \norm{v}_1^2
\end{align*}
This inequality can be solved for $h$ using the quadratic formula.
Let $\tilde{h}$ be the solution.
Then, letting $h_\star := \max(\tilde{h}, 0)$,
we have that $\varphi(h_{\star}) \geq 0$.

Next, we derive the upper bound $h^\star$.
Similar to the lower bound, we approximate~(\ref{eq:newton-abs:upper-bound-problem}).
Since
\begin{align}
    \sum\limits_{i=1}^p
    \frac{v_i^2}{(\Sigma_{ii} h + \lambda)^2}
    &=
    \sum\limits_{i: \Sigma_{ii} > 0}
    \frac{v_i^2}{(\Sigma_{ii} h + \lambda)^2}
    \leq 
    h^{-2}
    \sum\limits_{i: \Sigma_{ii} > 0}
    \frac{v_i^2}{\Sigma_{ii}^2 }
    \label{eq:nmab:upper-approx}
\end{align}
by setting 
\begin{align*}
    h^\star
    := 
    \sqrt{
        \sum\limits_{i: \Sigma_{ii} > 0} \frac{v_i^2}{\Sigma_{ii}^2}
    }
\end{align*}
we have that $\varphi(h^\star) \leq 0$.


%\section{EDPP Derivations}\label{appendix:edpp}
%
%We use the EDPP result as described in~\citet{wang:2015}
%to generalize the result for our objective with elastic net and arbitrary group-wise penalty~\labelcref{eq:glp:objective-general}.
%We note that we are still working under the assumption that the data matrix $X$ has been transformed
%as described in~\Cref{sec:optimizer}.
%
%We first transform the problem~\labelcref{eq:glp:objective-general} 
%using augmented data to pose it as the objective in~\citet{wang:2015}.
%The group-lasso problem that~\citet{wang:2015} solves is
%\begin{align}
%    \minimize_{\beta \in \R^p}
%    \frac{1}{2} \norm{y-X\beta}_2^2
%    + \lambda \sum\limits_{i=1}^G w_i \norm{\beta_i}_2
%    \label{eq:appendix:edpp:objective}
%\end{align}
%where $w_i := \sqrt{n_i}$ and $n_i$ is the size of group $i$.
%While the authors assumed this particular choice of $w_i$,
%their results generalize to any penalty with non-negative values. 
%Now, define the augmented data 
%\begin{align}
%    \tilde{X} :=
%    \begin{bmatrix}
%        X \\
%        \sqrt{\lambda (1-\alpha)} \sqrt{P}
%    \end{bmatrix}
%    ,
%    \quad
%    \tilde{y} :=
%    \begin{bmatrix}
%        y \\
%        0
%    \end{bmatrix}
%    ,
%    \quad
%    P
%    :=
%    \begin{bmatrix}
%        w_1 I_{p_1} & 0 & \ldots & 0 \\
%        0 & w_2 I_{p_2} & \ldots & 0 \\
%        \vdots & \vdots & \ddots & \vdots \\
%        0 & 0 & \ldots & w_G I_{p_G}  
%    \end{bmatrix}
%    ,
%    \quad
%    \tilde{\lambda} := \lambda \alpha
%    \label{eq:appendix:edpp:transformed-data}
%\end{align}
%Then, we write~\labelcref{eq:glp:objective-general} as
%\begin{align}
%    \minimize_{\beta \in \R^p}
%    \frac{1}{2} \norm{\tilde{y} - \tilde{X} \beta}_2^2
%    + \tilde{\lambda} \sum\limits_{i=1}^G w_i \norm{\beta_i}_2
%    \label{eq:appendix:edpp:transformed-objective}
%\end{align}
%which matches~\labelcref{eq:appendix:edpp:objective}.
%Without loss of generality, we may assume $w_i > 0$ for all $i$.
%Otherwise, for the groups with $w_i = 0$, 
%we may solve for $\beta^\star_i$ as an unpenalized optimization problem,
%which remains optimal for any $\lambda$.
%Hence, we may first compute the residual $y - \sum\limits_{i : w_i = 0} X_i \beta^\star_i$
%and optimize over this residual and the sub-data matrix containing only the feature blocks where $w_i > 0$.
%
%Next, following~\citet[Corollary 24 EDPP]{wang:2015},
%consider a given sequence of regularization parameters 
%$\lambda_{\max} = \lambda_0 > \lambda_1 > \ldots > \lambda_{\sK} > 0$
%to solve~\labelcref{eq:appendix:edpp:objective}.
%Here, $\lambda_{\max}$ is the smallest $\lambda$ such that 
%the solution at $\lambda_{\max}$ is the zero vector.
%Without loss of generality, we assume not all $w_i$ are zero,
%since otherwise, we simply have a least-squares problem.
%For any $0 \leq k < \sK$ and group $1 \leq g \leq G$,
%we have that $\beta_g^\star(\lambda_{k+1}) = 0$ if $\beta^\star(\lambda_k)$ is known and
%the EDPP condition is satisfied:
%\begin{align}
%    \norm{X_g^\top \pr{\theta^\star(\lambda_k) + \frac{1}{2} \overline{v}_2^\perp(\lambda_{k+1}, \lambda_k)}}_2
%    &<
%    w_g - \frac{1}{2} \norm{\overline{v}_2^\perp(\lambda_{k+1}, \lambda_k)}_2 \norm{X_g}_2
%    \label{eq:appendix:edpp:condition}
%    \tag{EDPP Condition}
%\end{align}
%where (with a slight abuse of notation)
%\begin{align}
%    \theta^\star(\lambda_0) 
%    &:= 
%    \lambda_0^{-1} \pr{y - X \beta^\star(\lambda_0)}
%    \label{eq:appendix:edpp:theta}
%    \\
%    \overline{v}_2^\perp(\lambda, \lambda_0)
%    &:=
%    \overline{v}_2(\lambda, \lambda_0)
%    -
%    \frac{\overline{v}_1(\lambda_0)^\top \overline{v}_2(\lambda, \lambda_0)}{\norm{\overline{v}_1(\lambda_0)}_2^2}
%    \overline{v}_1(\lambda_0)
%    \label{eq:appendix:edpp:v2perp}
%    \\
%    \overline{v}_2(\lambda, \lambda_0)
%    &:=
%    \lambda^{-1} y - \theta^\star(\lambda_0)
%    \label{eq:appendix:edpp:v2}
%    \\
%    \overline{v}_1(\lambda_0)
%    &:=
%    \begin{cases}
%        \lambda_0^{-1} y - \theta^\star(\lambda_0) ,& \lambda_0 \in (0, \lambda_{\max}) \\
%        X_{g_\star} X_{g_\star}^\top y ,& \lambda_0 = \lambda_{\max}
%    \end{cases}
%    \label{eq:appendix:edpp:v1}
%    \\
%    g_\star
%    &:= 
%    \argmax\limits_{1\leq g \leq G}
%    \frac{\norm{X_g^\top y}_2}{w_g}
%    \label{eq:appendix:edpp:xstar}
%    \\
%    \lambda_{\max}
%    &:=
%    \frac{\norm{X_{g_\star}^\top y}_2}{w_{g_\star}}
%    \label{eq:appendix:edpp:lmdamax}
%\end{align}
%Similarly, define $\tilde{\theta}^\star$,
%$\tilde{\overline{v}}_2^\perp$,
%$\tilde{\overline{v}}_2$,
%and $\tilde{\overline{v}}_1$.
%where all quantities are replaced with their tilde versions
%using the augmented data~\labelcref{eq:appendix:edpp:transformed-data}
%to solve~\labelcref{eq:appendix:edpp:transformed-objective}.
%Note that $g_\star$ remains invariant under this data transformation,
%however, $\tilde{\lambda}_{\max} = w_{g_\star}^{-1} \norm{X_{g_\star}^\top y}_2$.
%On the original scale, $\lambda_{\max} \equiv \alpha^{-1} \tilde{\lambda}_{\max}$.
%
%Our goal is to simplify~\labelcref{eq:appendix:edpp:condition}
%for the transformed problem~\labelcref{eq:appendix:edpp:transformed-objective}
%using quantities from~\labelcref{%
%eq:appendix:edpp:theta,%
%eq:appendix:edpp:v2perp,%
%eq:appendix:edpp:v2,%
%eq:appendix:edpp:v1,%
%eq:appendix:edpp:xstar,%
%eq:appendix:edpp:lmdamax%
%}.
%To generalize the discussion, we define $\lambda_0$ to be the point at which
%we know the solution $\beta^\star(\lambda_0)$ and that implicitly defines the transformed data.
%We let $\lambda$ be the new regularization at which we wish to deduce the zero coefficients.
%Note that we work in the $\tilde{\lambda}$ space so that
%\begin{align*}
%    \tilde{X}
%    \equiv
%    \begin{bmatrix}
%        X \\
%        \sqrt{\lambda_0 \alpha^{-1} (1-\alpha)} \sqrt{P}
%    \end{bmatrix}
%\end{align*}
%First,
%\begin{align*}
%    \tilde{X}^\top \tilde{\theta}^\star(\lambda_0)
%    &=
%    \lambda_0^{-1} (\tilde{X}^\top \tilde{y} - \tilde{X}^\top \tilde{X} \beta^\star(\lambda_0))
%    \\&=
%    \lambda_0^{-1} \pr{X^\top y - \pr{X^\top X + \lambda_0 \alpha^{-1} (1-\alpha) P} \beta^\star(\lambda_0)}
%    \\&=
%    X^\top \theta^\star(\lambda_0)
%    -
%    \alpha^{-1} (1-\alpha) P \beta^\star(\lambda_0)
%\end{align*}
%Next,
%\begin{align*}
%    \tilde{X}^\top \tilde{\overline{v}}_2(\lambda, \lambda_0)
%    &=
%    \lambda^{-1} X^\top y - \tilde{X}^\top \tilde{\theta}^\star(\lambda_0)
%    \\&=
%    X^\top \overline{v}_2(\lambda, \lambda_0)
%    +
%    \alpha^{-1} (1-\alpha) P \beta^\star(\lambda_0)
%    \\
%    \tilde{X}^\top \tilde{\overline{v}}_1(\lambda_0)
%    &=
%    \begin{cases}
%        X^\top \overline{v}_1(\lambda_0) + \alpha^{-1} (1-\alpha) P\beta^\star(\lambda_0) ,& \lambda_0 \in (0, \tilde{\lambda}_{\max}) \\
%        \tilde{X}^\top \tilde{X}_{g_\star} X_{g_\star}^\top y ,& \lambda_0 = \tilde{\lambda}_{\max}
%    \end{cases}
%    \\
%    \tilde{X}_i^\top \tilde{X}_j
%    &=
%    \begin{cases}
%        X_i^\top X_i + \lambda_0 \alpha^{-1} (1-\alpha) w_i I ,&  i=j \\
%        X_i^\top X_j ,&  i\neq j
%    \end{cases}
%\end{align*}
%Now, we simplify $\tilde{\overline{v}}_1(\lambda_0)^\top \tilde{\overline{v}}_2(\lambda,\lambda_0)$.
%Consider first the case when $\lambda_0 = \lambda_{\max}$.
%\begin{align*}
%    \tilde{\overline{v}}_1(\lambda_0)^\top 
%    \tilde{\overline{v}}_2(\lambda, \lambda_0)
%    &=
%    \lambda^{-1} \norm{X_{g_\star}^\top y}_2^2
%    - 
%    y^\top X_{g_\star} \tilde{X}_{g_\star}^\top \tilde{\theta}^\star(\lambda_0)
%    \\&=
%    \overline{v}_1(\lambda_0)^\top \overline{v}_2(\lambda,\lambda_0)
%    +
%    w_{g_\star} \alpha^{-1} (1-\alpha) y^\top X_{g_\star} \beta^\star_{g_\star}(\lambda_0)
%\end{align*}
%Next, if $\lambda_0 \in (0, \lambda_{\max})$,
%\begin{align*}
%    \tilde{\overline{v}}_1(\lambda_0)^\top 
%    \tilde{\overline{v}}_2(\lambda, \lambda_0)
%    &=
%    \overline{v}_1(\lambda_0)^\top
%    \overline{v}_2(\lambda,\lambda_0)
%    +
%    \lambda_0^{-1} \alpha^{-1} (1-\alpha) \beta^\star(\lambda_0)^\top P \beta^\star(\lambda_0)
%\end{align*}
%Finally,
%\begin{align*}
%    \norm{\tilde{\overline{v}}_1(\lambda_0)}_2^2
%    &=
%    \begin{cases}
%        \norm{\overline{v}_1(\lambda_0)}_2^2 + 
%        \lambda_0^{-1} \alpha^{-1} (1-\alpha) \beta^\star(\lambda_0)^\top P \beta^\star(\lambda_0)
%        ,& \lambda_0 \in (0, \tilde{\lambda}_{\max}) \\
%        \norm{X_{g_\star} X_{g_\star}^\top y}_2^2 ,& \lambda_0 = \tilde{\lambda}_{\max}
%    \end{cases}
%\end{align*}
%Thus, these formulas simplify~\labelcref{eq:appendix:edpp:condition} for the augmented data case.
%Note also that
%\begin{align*}
%    \norm{\tilde{X}_g}_2^2
%    &=
%    \trace\pr{\tilde{X}_g^\top \tilde{X}_g}
%    =
%    \norm{X_g}_2^2 + \lambda_0 \alpha^{-1} (1-\alpha) w_g p_g
%\end{align*}

\section{EDPP Derivations}

Recall that the group lasso problem in full generality is given by
\begin{align}
    \minimize\limits_{\beta \in \R^p}
    \frac{1}{2}
    \norm{y - X\beta}_2^2 
    + 
    \lambda \sum\limits_{i=1}^G w_i \pr{\alpha \norm{\beta_i}_2 + \frac{1-\alpha}{2} \norm{\beta_i}_2^2}
    \label{eq:appendix:edpp:objective}
\end{align}
We assume that $\alpha \in (0,1)$ and all $w_i > 0$.
If $\alpha = 0$, then there is no point of discussing ways to remove variables since
all variables must be active.
If $\alpha = 1$, then the EDPP rule is given by~\citet[Corollary 24]{wang:2015}.
If $w_i = 0$ for some $i$, then we may first optimize over such $\beta_i$,
which is an unpenalized optimization problem whose solutions do not depend on $\lambda$.
Afterwards, we may replace $y$ by the residual
and $X$ by the sub-matrix containing only the group features
whose corresponding $w_i > 0$.

Let $z = y - X\beta$ so that~\labelcref{eq:appendix:edpp:objective} becomes
\begin{align}
    &\minimize\limits_{\beta \in \R^p, z \in \R^n}
    \frac{1}{2} \norm{z}_2^2
    +
    \lambda \sum\limits_{i=1}^G w_i \pr{\alpha \norm{\beta_i}_2 + \frac{1-\alpha}{2} \norm{\beta_i}_2^2}
    \\
    &\subjto \quad z = y - X\beta
    \label{eq:appendix:edpp:objective-transformed}
\end{align}
We introduce the dual variables $\eta \in \R^n$ such that 
the Lagrangian of~\labelcref{eq:appendix:edpp:objective-transformed}
is given by
\begin{align*}
    L(\beta, z, \eta)
    :=
    \frac{1}{2} \norm{z}_2^2 
    +
    \lambda \sum\limits_{i=1}^G w_i \pr{\alpha \norm{\beta_i}_2 + \frac{1-\alpha}{2} \norm{\beta_i}_2^2}
    + \eta^\top \pr{y - X\beta - z}
\end{align*}
Thus, the dual function $g(\eta)$ is given by
\begin{align*}
    g(\eta)
    &:=
    \inf\limits_{\beta, z} L(\beta, \eta)
    \\&=
    \eta^\top y + 
    \inf\limits_{\beta} 
    \pr{-\eta^\top X \beta + \lambda \sum\limits_{i=1}^G w_i \pr{\alpha \norm{\beta_i}_2 + \frac{1-\alpha}{2} \norm{\beta_i}_2^2}}
    +
    \inf\limits_{z}
    \pr{
        \frac{1}{2} \norm{z}_2^2 - \eta^\top z
    }
\end{align*}
We now solve for the two minimization problems.
First, define
\begin{align*}
    \varphi(\beta)
    &:=
    -\eta^\top X \beta + \lambda \sum\limits_{i=1}^G w_i \pr{\alpha \norm{\beta_i}_2 + \frac{1-\alpha}{2} \norm{\beta_i}_2^2}
    \\&=
    \sum\limits_{i=1}^G
    \pr{
        -\eta^\top X_i \beta_i + \lambda w_i \pr{\alpha \norm{\beta_i}_2 + \frac{1-\alpha}{2} \norm{\beta_i}_2^2}
    }
    =:
    \sum\limits_{i=1}^G \varphi_i(\beta_i)
\end{align*}
Since $\varphi$ is separable, we minimize each $\varphi_i$ separately.
For any fixed $i$, the optimal point $\beta_i^\star$ for minimizing $\varphi_i$ satisfies
\begin{align}
    -X_i^\top \eta + \lambda \alpha w_i u_i + \lambda (1-\alpha) w_i \beta_i = 0
    \label{eq:appendix:edpp:subg1}
\end{align}
where 
\begin{align*}
    u_i
    &\in
    \begin{cases}
        \set{\frac{\beta_i}{\norm{\beta_i}_2}} ,& \beta_i \neq 0 \\
        \set{u \in \R^{p_i} : \norm{u}_2 \leq 1} ,& \beta_i = 0
    \end{cases}
\end{align*}
If $\beta_i^\star = 0$, note that $\varphi_i(\beta_i^\star) = 0$.
Otherwise, 
\begin{align*}
    \varphi_i(\beta_i)
    &=
    \beta_i^\top \pr{
        -X_i^\top \eta + \lambda \alpha w_i \frac{\beta_i}{\norm{\beta_i}_2} + \lambda \frac{1-\alpha}{2} w_i \beta_i   
    }
    \\
    \implies
    \varphi_i(\beta_i^\star)
    &=
    -\lambda \frac{1-\alpha}{2} w_i \norm{\beta_i^\star}_2^2
\end{align*}
Now, recall that we also have from~\labelcref{eq:appendix:edpp:subg1} that
\begin{align*}
    \lambda w_i \pr{\frac{\alpha}{\norm{\beta_i^\star}_2} + (1-\alpha)} \beta_i^\star
    &=
    X_i^\top \eta
    \\\implies
    \norm{\beta_i^\star}_2
    &=
    \frac{\norm{X_i^\top \eta}_2}{\lambda (1-\alpha) w_i} - \frac{\alpha}{1-\alpha}
    \\&=
    \frac{\norm{X_i^\top \eta}_2 - \lambda \alpha w_i}{\lambda (1-\alpha) w_i}
\end{align*}
Note that this also gives us
\begin{align*}
    \beta_i^\star
    &=
    \frac{1}{\lambda (1-\alpha) w_i}
    \pr{1 - \frac{\lambda \alpha w_i}{\norm{X_i^\top \eta}_2}}
    X_i^\top \eta
\end{align*}
Hence, in either case, we have that
\begin{align*}
    \varphi_i(\beta_i^\star)
    =
    -\frac{\pr{\norm{X_i^\top \eta}_2 - \lambda \alpha w_i}_+^2}{2\lambda (1-\alpha) w_i}
\end{align*}
Note that unlike when $\alpha = 1$, we do not have any constraints on $\eta$ when $\alpha \in (0,1)$.
This shows that
\begin{align*}
    \inf\limits_{\beta}
    \varphi(\beta)
    &=
    -\frac{1}{2\lambda (1-\alpha)}
    \sum\limits_{i=1}^G
    \frac{\pr{\norm{X_i^\top \eta}_2 - \lambda \alpha w_i}_+^2}{w_i}
\end{align*}

Next,
\begin{align*}
    \inf\limits_{z} \frac{1}{2} \norm{z}_2^2 - \eta^\top z
    =
    - \frac{1}{2} \norm{\eta}_2^2
\end{align*}

Combining, the dual function simplifies to
\begin{align*}
    g(\eta)
    &=
    \eta^\top y 
    - \frac{1}{2} \norm{\eta}_2^2
    -\frac{1}{2\lambda (1-\alpha)}
    \sum\limits_{i=1}^G
    \frac{\pr{\norm{X_i^\top \eta}_2 - \lambda \alpha w_i}_+^2}{w_i}
\end{align*}
So, the dual formulation is
\begin{align*}
    &\maximize\limits_{\eta} 
    \eta^\top y 
    - \frac{1}{2} \norm{\eta}_2^2
    -\frac{1}{2\lambda (1-\alpha)}
    \sum\limits_{i=1}^G
    \frac{\pr{\norm{X_i^\top \eta}_2 - \lambda \alpha w_i}_+^2}{w_i}
\end{align*}
which is equivalent to
\begin{align*}
    &\maximize\limits_{\eta} 
    \frac{1}{2} \norm{y}_2^2
    - \frac{1}{2} \norm{\eta-y}_2^2
    -\frac{1}{2\lambda (1-\alpha)}
    \sum\limits_{i=1}^G
    \frac{\pr{\norm{X_i^\top \eta}_2 - \lambda \alpha w_i}_+^2}{w_i}
\end{align*}
By a rescaling of the dual variables $\eta$ such that $\theta = \lambda^{-1} \eta$,
the dual formulation can be written as 
\begin{align}
    \maximize\limits_{\theta} 
    \frac{1}{2} \norm{y}_2^2
    - \frac{\lambda^2}{2} \norm{\theta-\frac{y}{\lambda}}_2^2
    -\frac{\lambda}{2(1-\alpha)}
    \sum\limits_{i=1}^G
    \frac{\pr{\norm{X_i^\top \theta}_2 - \alpha w_i}_+^2}{w_i}
    \label{eq:appendix:edpp:final}
\end{align}
Following the proof in~\citet[Appendix B.2]{wang:2015},
the KKT conditions are given by
\begin{align*}
    y &= X\beta^\star(\lambda) + \lambda \theta^\star(\lambda) \\
    X_i^\top \theta^\star(\lambda) - (1-\alpha) w_i \beta^\star_i(\lambda)
    &\in
    \begin{cases}
        \set{\alpha w_i \frac{\beta_i^\star(\lambda)}{\norm{\beta_i^\star(\lambda)}_2}} ,& \beta_i^\star(\lambda) \neq 0 \\
        \set{\alpha w_i u : \norm{u}_2 \leq 1} ,& \beta_i^\star(\lambda) = 0 
    \end{cases}
\end{align*}

It is difficult to devise a safe screening rule because of the penalty in~\labelcref{eq:appendix:edpp:final}.
Specifically, if $\alpha < 1$, then we have an unconstrained problem that tends to pull more coefficients to be non-zero.
The penalty becomes a bona fide barrier function when $\alpha \uparrow 1$.
This is precisely the case that~\citet{wang:2015} studies.
When $\alpha = 1$, the maximization can only occur non-trivially if 
$\norm{X_i^\top \theta} \leq \lambda w_i$.
This constraint also agrees with the derivation in~\citet{wang:2015}.
In such a case, the maximization problem is simply a projection onto a closed, non-empty convex set,
which was the critical observation.
This cannot generalize when $\alpha < 1$.

\section{Parallel Block Coordinate Descent}

In this section, we describe a new algorithm for minimizing a loss function 
$f : \R^p \to \overline{\R}$ of the form
\begin{align}
    f(x)
    &=
    f_0(x)
    +
    \sum\limits_{i=1}^G f_i(x_i) 
    \label{eq:bcd:separability}
\end{align}
where $f_0 : \R^p \to \overline{\R}$ is a smooth function,
$f_i: \R^{p_i} \to \overline{\R}$ is any arbitrary (measurable) function
that only depends on a block $x_i$,
and $x = (x_1,\ldots, x_G) \in \R^p$ where $x_i \in \R^{p_i}$.
Note that we are in the same setup as in~\citet{tseng:2001}.

\begin{algorithm}[t]
    \caption{Parallel Block Coordinate Descent}\label{alg:parallel-bcd}
    \KwData{$x^{(0)}$}
    \For{$k=0,1,\ldots$} {
        \For{$i=1,\ldots,G$ in parallel} {
            $y_i^{(k)} \gets \argmin\limits_{x_i} f(x^{(k)}_1,\ldots, x_i, \ldots, x^{(k)}_G)$\;
        }
        \For{$i=1,\ldots,G$ in parallel} {
            compute $\alpha_i^{(k)}$\;
            $x_i^{(k+1)} \gets (1-\alpha_i^{(k)}) x_i^{(k)} + \alpha_i^{(k)} y_i^{(k)}$\;
        }
    }
    return $x^{\star}$\;
\end{algorithm}

We describe our algorithm for parallel block coordinate descent
given in~\Cref{alg:parallel-bcd}.
Suppose at step $k \geq 0$, we are given a point $x^{(k)}$.
The usual cyclic coordinate descent iterates through each block of coefficients 
\emph{sequentially} in the following manner:
\begin{align*}
    x_i^{(k+1)} &= \argmin\limits_{x_i} f(x^{(k+1)}_1,\ldots, x_i, \ldots, x^{(k)}_G)
\end{align*}
where the current block $i$ is optimized while fixing other blocks at their current updates. 
This dependence across the block updates makes this algorithm impossible to parallelize.
However, with a slight modification to the algorithm, it becomes easily parallelizable.

Consider the following new update scheme:
\begin{align}
    y_i^{(k)} &= \argmin\limits_{x_i} f(x^{(k)}_1,\ldots, x_i, \ldots, x^{(k)}_G) 
    \label{eq:parallel-bcd:parallel-update}
    \\
    x_i^{(k+1)} &= (1-\alpha_i^{(k)}) x^{(k)}_i + \alpha_i^{(k)} y^{(k)}_i
    \label{eq:parallel-bcd:combine-update}
\end{align}
where we let $\alpha_i^{(k)} \in [0,1]$ be any set of weights for now.
Note the key difference that
$y_i^{(k)}$ as in~\labelcref{eq:parallel-bcd:parallel-update}
can be computed \emph{in parallel},
since we fix all other coordinates at $x^{(k)}$ rather than at their new updates.
To give convergence guarantees, we must impose some conditions on $f$
and $\alpha_i^{(k)}$, however, these conditions hold for the group lasso objective~\labelcref{eq:glp:objective-general}.
More generally, \Cref{thm:parallel-bcd} shows that
every cluster point of $\set{x^{(k)}}_{k=0}^\infty$
converges to a coordinate-wise minimum point under some mild conditions.

\begin{theorem}[Properties of Parallel BCD]\label{thm:parallel-bcd}
Consider the sequences $\set{x^{(k)}}_{k=0}^\infty$ and $\set{y^{(k)}}_{k=0}^\infty$
defined by~\labelcref{eq:parallel-bcd:combine-update,eq:parallel-bcd:parallel-update}
where $\alpha_i^{(k)} \in [0,1]$.
Assume that $f$ is convex.
Suppose also that $\sum\limits_{i=1}^G \alpha_i^{(k)} = 1$
for each $k \geq 0$.
Then, we have the following guarantees:
\begin{enumerate}[label=(\roman*)]
\item\label{thm:parallel-bcd:descent}
    $x^{(k)}$ is a bona fide descent, i.e. $f(x^{(k+1)}) \leq f(x^{(k)})$.
    
\item\label{thm:parallel-bcd:convg}
    Suppose that for each $i$,
    $\liminf\limits_{k\to\infty} \alpha_i^{(k)} > 0$.
    Assume that the level sets $\sL_a := \set{x : f(x) \leq f(a)}$ are compact.
    Then, every cluster point of $\set{x^{(k)}}_{k=0}^\infty$ is a coordinate-wise minimum point.
    That is, if $x^{(n_k)} \to x^\star$ along a subsequence $n_k$, 
    then for any $i =1,\ldots, G$ and $d_i \in \R^{p_i}$,
    \begin{align*}
        f(x^\star) \leq f(x^\star + \pi_i(d_i))
    \end{align*}
    where $\pi_i(d_i) := (0,\ldots, d_i, \ldots, 0)$.
\end{enumerate}
\end{theorem}
\begin{proof}

Let $\pi_i: \R^{p_i} \to \R^p$ such that $\pi_i(d) = (0,\ldots, d,\ldots, 0)$
so that $d \in \R^{p_i}$ is embedded in the $i$th block.

We first prove~\labelcref{thm:parallel-bcd:descent}.
Fix any $k\geq 0$ and consider $x^{(k)}$.
By definition of $y^{(k)}$, we have for each $i=1,\ldots, G$ that
\begin{align*}
    f(x^{(k)} + \pi_i(y_i^{(k)} - x_i^{(k)}))
    \leq
    f(x^{(k)})
\end{align*}
Hence, by hypothesis that $\sum\limits_{i=1}^G \alpha_i^{(k)} = 1$,
\begin{align*}
    \sum\limits_{i=1}^G 
    \alpha_i^{(k)}
    f(x^{(k)} + \pi_i(y_i^{(k)} - x_i^{(k)}))
    \leq
    f(x^{(k)})
\end{align*}
Next, by convexity of $f$, 
\begin{align*}
    f\pr{
        x^{(k)}
        +
        \sum\limits_{i=1}^G
        \alpha_i^{(k)}
        \pi_i(y_i^{(k)} - x_i^{(k)})
    }
    \leq
    \sum\limits_{i=1}^G 
    \alpha_i^{(k)}
    f(x^{(k)} + \pi_i(y_i^{(k)} - x_i^{(k)}))
    \leq
    f(x^{(k)})
\end{align*}
Upon inspection, we see that
\begin{align*}
    x^{(k+1)}
    &=
    x^{(k)}
    +
    \sum\limits_{i=1}^G
    \alpha_i^{(k)}
    \pi_i(y_i^{(k)} - x_i^{(k)})
\end{align*}
Hence, we have shown that $f(x^{(k+1)}) \leq f(x^{(k)})$,
that is, we have a descent method.

We now prove~\labelcref{thm:parallel-bcd:convg}.
Note that since the level sets are compact 
and~\labelcref{thm:parallel-bcd:descent} has been established, $x^{(k)}$ is bounded.
Consider any convergent subsequence $x^{(n_k)} \to x^\star$.

We first begin with an inequality that holds for any $k \geq 0$.
For any $j=1,\ldots, G$ and $d_j \in \R^{p_j}$,
\begin{align}
    f(x^{(k+1)})
    &=
    f\pr{x^{(k)} + \sum\limits_{i=1}^G \alpha_i^{(k)} \pi_i(y_i^{(k)}-x_i^{(k)})}
    \nonumber
    \\&\leq
    \sum\limits_{i=1}^G
    \alpha_i^{(k)}
    f(x^{(k)} + \pi_i(y_i^{(k)}-x_i^{(k)}))
    \nonumber
    \\&\leq
    \sum\limits_{i\neq j}
    \alpha_i^{(k)}
    f(x^{(k)} + \pi_i(y_i^{(k)}-x_i^{(k)}))
    +
    \alpha_j^{(k)}
    f(x^{(k)} + \pi_j(d_j))
    \nonumber
    \\&\leq
    (1-\alpha_j^{(k)}) f(x^{(k)})
    + 
    \alpha_j^{(k)}
    f(x^{(k)} + \pi_j(d_j))
    \label{eq:thm:parallel-bcd:key-inequality}
\end{align}
where we used convexity and that $y_i^{(k)}$ minimizes along the $i$th coordinate
with all other coordinates fixed at $x^{(k)}$.

Now, since $f(x^{(k)})$ is monotonically decreasing by~\labelcref{thm:parallel-bcd:descent}
and $n_{k+1} \geq n_k+1$,
we have that
\begin{align*}
    f(x^{(n_{k+1})}) \leq f(x^{(n_k+1)}) \leq f(x^{(n_k)})
\end{align*}
Since $k\mapsto f(x^{n_k+1})$ is also monotonically decreasing,
taking limit as $k\to\infty$ shows that
\begin{align*}
    f(x^\star) \leq \lim\limits_{k\to\infty} f(x^{(n_k+1)}) \leq f(x^\star)
\end{align*}
by continuity of $f$, which is implied by convexity.

By possibly passing through a further subsequence of $n_k$,
we may assume without loss of generality that $\alpha_i^{(n_k)}$ converges
to a finite limit $\alpha_i^\star$ for every $i=1,\ldots, G$.
Since $\liminf\limits_{k\to\infty} \alpha_i^{(k)} > 0$,
we have that $\alpha_i^\star > 0$ for every $i=1,\ldots, G$.
Then, taking limit as $k\to\infty$ in~\labelcref{eq:thm:parallel-bcd:key-inequality}
restricted to the subsequence $n_k$,
\begin{align*}
    f(x^\star)
    &\leq 
    (1-\alpha_j^\star) f(x^\star)
    + \alpha_j^\star f(x^\star + \pi_j(d_j))
    \\ \implies
    f(x^\star)
    &\leq
    f(x^\star + \pi_j(d_j))
\end{align*}
Since this is true for any $j$ and $d_j$,
by definition, we have that $x^\star$ is a coordinate-wise minimum point.
\end{proof}

We emphasize that in~\Cref{thm:parallel-bcd},
one can choose $\alpha_i^{(k)}$ 
in \emph{any fashion} so long as the conditions are satisfied.
In particular, one may choose $\alpha_i^{(k)}$ \emph{adaptively} based on the current iterate $x^{(k)}$.

Turning to our original discussion of the group lasso objective~\labelcref{eq:glp:objective-general},
we give a convergence guarantee using~\Cref{thm:parallel-bcd} in~\Cref{cor:parallel-bcd}. 

\begin{corollary}[Convergence of Parallel BCD for Group Lasso]
\label{cor:parallel-bcd}
Let $\alpha_i^{(k)} \in [0,1]$ be such that $\liminf\limits_{k\to\infty} \alpha_i^{(k)} > 0$
for every $i=1,\ldots, G$ and $\sum\limits_{i=1}^G \alpha_i^{(k)}$ for all $k\geq 0$.
If the group lasso objective~\labelcref{eq:glp:objective-general}
admits a unique minimizer, then the parallel BCD iterates $\set{x^{(k)}}$ 
given by~\labelcref{eq:parallel-bcd:parallel-update,eq:parallel-bcd:combine-update}
converges to the global minimium.
\end{corollary}
\begin{proof}

Note that the group lasso objective~\labelcref{eq:glp:objective-general}
is indeed convex and has compact level sets.
\Cref{thm:parallel-bcd} then shows that every cluster point of $\set{x^{(k)}}_{k=0}^\infty$
is a coordinate-wise minimum point.
Following~\citet[Lemma 3.1]{tseng:2001}, since the objective also 
satisfies the separability condition~\labelcref{eq:bcd:separability}
with a differentiable component $f_0$, the objective is \emph{regular}.
Hence, every coordinate-wise minimum point $z$ is a stationary point,
that is, $f'(z; d) \geq 0$ for all $d \in \R^p$.
By convexity, we have that $z$ is a global minimum.
As soon as the group lasso objective admits a unique minimizer,
the sequence $\set{x^{(k)}}_{k=0}^\infty$ then
converges to the global minimum.

\end{proof}
