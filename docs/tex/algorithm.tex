\section{Optimizer Algorithm}\label{sec:optimizer}

In this section, we discuss the algorithm to solve the optimization problem~(\ref{eq:glp:objective}).
Let $X \in \R^{n\times p}$, $y\in \R^n$ be the feature matrix and response vector, respectively.
Let $G$ be the number of groups and $p_i$ be the size of group $i$ for $1\leq i\leq G$.
Further let $X = \br{X_1 \, X_2 \, \ldots \, X_G}$ where $X_i \in \R^{n \times p_i}$ is the feature matrix
corresponding to group $i$ features only.

In general, if the loss function $f$ is of the form $f(\beta) \equiv f(X\beta)$,
the optimization problem~(\ref{eq:glp:objective}) can be reparametrized
in terms of $\tilde{\beta} \in \R^p$ defined by $\tilde{\beta}_i := V_i^\top \beta_i$,
where $V_i \in \R^{p_i \times p_i}$ is the orthogonal matrix from Singular Value Decomposition (SVD) 
of $X_i$. 
Specifically, for each $i=1,\ldots, G$, we decompose $X_i = U_i D_i V_i^\top$
where $U_i \in \R^{n \times m_i}$, $D_i \in \R^{m_i \times p_i}$, and $V_i \in \R^{p_i \times p_i}$
with $m_i := \min(n, p_i)$.
We will shortly see that this parametrization leads to a much simpler optimizing algorithm.\todojames{ref sec} 
Concretely, it suffices to solve
\begin{align}
    \minimize_{\tilde{\beta} \in \R^p}
    f(\tilde{X} \tilde{\beta})
    + \lambda \sum\limits_{j=1}^G \norm{\tilde{\beta}_j}_2
    \label{eq:oa:new-objective}
\end{align}
where $\tilde{X} = \br{X_1 V_1 \, X_2 V_2 \, \ldots \, X_G V_G}$.
Note that this was only possible because the group lasso penalty is invariant under
the transformation from $\beta \mapsto \tilde{\beta}$.
Thus, given a solution $\tilde{\beta}^*$ to the new problem~(\ref{eq:oa:new-objective}),
we have the solution to the original problem $\beta^*$ given by
$\beta^*_i := V_i \tilde{\beta}_i^*$.
Hence, without loss of generality, we assume that 
our feature matrix $X$ is of the form $\tilde{X}$.
In particular, we assume that $X_i^\top X_i = D_i^\top D_i =: \Sigma_i$ is diagonal for all $1\leq i\leq G$.
Henceforth, we drop all tilde for notational simplicity.

Computationally, it may seem daunting to 
construct $\tilde{X}$ from $X$ before solving~(\ref{eq:oa:new-objective}).
However, the time taken for this pre-processing is relatively cheap compared to the full optimizing algorithm.
Given the data matrix $X$, consider the SVD of block $X_i \in \R^{n \times p_i}$.
If $n \geq p_i$, then $\tilde{X}_i$ can be computed in $O(n p_i)$ time, noting that $D_i$ is diagonal.
Otherwise if $n < p_i$, then $D_i = [D_{i0} \; \textbf{0}_{n\times (p-n)}]$ where $D_{i0} \in \R^{n \times n}$ is a diagonal matrix.
Then, $\tilde{X}_i = [U_i D_{i0} \; \textbf{0}_{n \times (p-n)}]$ so that the only product to be computed takes $O(n^2)$ time.
Hence, the total complexity of the procedure is
\begin{align*}
    O\pr{
        \sum\limits_{i=1}^G
        \pr{\underbrace{\min(n,p_i) n p_i}_{\svd} + \underbrace{\min(n, p_i) n}_{\tilde{X}_i}}
    }
    =
    O\pr{
        n
        \sum\limits_{i=1}^G
        \min(n,p_i) p_i
    }
\end{align*}
Moreover, since this algorithm is parallelizable across groups,
we can reduce the time by the number of parallel threads.
From numerical experimentation, this pre-processing takes less than $1\%$ of the total fitting time.
Thus, we view this pre-processing worthwhile since its cost is cheap
while inducing immense speed-ups in the optimizing algorithm.

In comparison to other proposed fitting procedures,
we believe that the extra cost to transform our $X$ matrix
is comparable to the cost of the other procedures.
Further, our transformation of $X$ is more general than existing methods.
For example,~\citet{yuan:2006,meier:2008} rely on the fact that
each group feature matrix $X_i$ is full-rank and therefore admits a QR decomposition with an invertible $R$.
Under this assumption, they solve~(\ref{eq:oa:new-objective}) with $\tilde{X} = \br{Q_1 \, Q_2 \, \ldots \, Q_G}$
and transform the solution back to the original scale by applying $R_i^{-1}$ to the $i$th group of coefficients.
The benefit of this approach is that the algorithm is aided by a simple closed-form expression
as part of the blockwise coordinate descent algorithm.
However, there are two major downsides to this approach.
The first is that, while the aforementioned authors primarily focused on the application of group lasso 
on multi-level categorical data that easily satisfy the full-rank assumption,
this assumption may easily be violated in other applications~\citep{simon:2012}.
The second is that this method only gives an approximation to the original problem.
That is, given the solution to~(\ref{eq:oa:new-objective}), $\tilde{\beta}^*$,
the candidate $\beta^\star$ defined by $\beta^\star_i := R_i^{-1} \tilde{\beta^\star}_i$ 
for $i=1,\ldots, G$ is \emph{not} necessarily a solution to~(\ref{eq:glp:objective})
with the original data.
Indeed, for this approximation to be exact, 
rather than solving~(\ref{eq:oa:new-objective}),
one should solve
\begin{align*}
    \minimize_{\beta \in \R^p}
    f(\tilde{X} \tilde{\beta})
    + \lambda \sum\limits_{j=1}^G \norm{R_i^{-1} \tilde{\beta}_j}_2
\end{align*}
which introduces a Tikhonov regularization.
Although this remains a convex optimization problem and can be solved in theory,
it does not simplify any computation for the optimizer.

Similar to other approaches~\citep{yuan:2006,meier:2008,tseng:2001,sparsegl:2022},
we use blockwise coordinate descent for its excellent speed and simplicity.
However, we take a completely different approach to fitting each block coordinate update.
The essence of implementing a highly performant optimizer lies in 
solving each block update~(\ref{eq:bcd:block-update}) as fast as possible.
Aside from the orthonormalization method described above as in~\citet{yuan:2006,meier:2008},
many works in literature that describe group lasso fitting procedures
use Proximal Gradient Descent (PGD), 
or Iterative Soft-Thresholding Algorithm (ISTA) in this context,
to solve~(\ref{eq:bcd:block-update})~\citep{sparsegl:2022,beck:2009,klosa:2020,wright:2009,loris:2009,sls:2016,odonoghue:2015}.
A Nesterov acceleration can be applied on ISTA to get Fast-ISTA (FISTA)~\citep{beck:2009}.
Further,~\citet{odonoghue:2015} showed that mild improvements can be made with an adaptive restarting strategy.
\Cref{ssec:bcd} introduces the blockwise coordinate descent algorithm in the context of
solving the group lasso problem.
\Cref{ssec:pgd} describes the PGD (ISTA) procedure and its variations to solve each block coordinate update.
In contrast to PGD,
\Cref{ssec:newton,ssec:newton-abs} describe our method to solve~(\ref{eq:bcd:block-update}).

\input{algorithm/bcd}
\input{algorithm/pgd}
\input{algorithm/newton}
\input{algorithm/newton-abs}