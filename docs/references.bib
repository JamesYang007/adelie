@article{tseng:2001,
    author = {Tseng, P.},
    title = {Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization},
    journal = {Journal of Optimization Theory and Applications},
    year = {2001}, 
    volume = {109},
    pages = {475-494},
    doi = {https://doi-org.stanford.idm.oclc.org/10.1023/A:1017501703105}
}

@article{odonoghue:2015,
    author = {O'Donoghue, Brendan and Cand\'{e}s, Emmanuel},
    title = {Adaptive Restart for Accelerated Gradient Schemes},
    journal = {Foundations of Computational Mathematics},
    year = {2015},
    volume = {15},
    pages = {715-732},
    doi = {https://doi-org.stanford.idm.oclc.org/10.1007/s10208-013-9150-3}
}

@book{sls:2016,
  author = {Hastie, Trevor and Tibshirani, Rob and Wainwright, Martin},
  title = {Statistical Learning with Sparsity},
  subtitle = {The Lasso and Generalizations},
  year = {2016},
  publisher = {Chapman \& Hall},
}

@misc{sparsegl:2022,
  doi = {10.48550/ARXIV.2208.02942},
  url = {https://arxiv.org/abs/2208.02942},
  author = {Liang, Xiaoxuan and Cohen, Aaron and Heinsfeld, Anibal Solón and Pestilli, Franco and McDonald, Daniel J.},
  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {sparsegl: An R Package for Estimating Sparse Group Lasso},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{beck:2009,
    author = {Beck, Amir and Teboulle, Marc},
    title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
    journal = {SIAM Journal on Imaging Sciences},
    volume = {2},
    number = {1},
    pages = {183-202},
    year = {2009},
    doi = {10.1137/080716542},
    URL = {https://doi.org/10.1137/080716542},
    eprint ={ https://doi.org/10.1137/080716542},
    abstract = { We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude. }
}

@article{meier:2008,
    author = {Meier, Lukas and Van De Geer, Sara and B\"{o}hlmann, Peter},
    title = {The group lasso for logistic regression},
    journal = {Journal of the Royal Statistical Society B},
    volume = {70},
    number = {1},
    pages = {53-71},
    year = {2008},
    doi = {10.1111/j.1467-9868.2007.00627.x}
}

@article{yuan:2006,
    author = {Yuan, Ming and Lin, Yi},
    title = {Model selection and estimation in regression with grouped variables},
    journal = {Journal of the Royal Statistical Society B},
    volume = {68},
    number = {1},
    pages = {49-67},
    year = {2006},
    doi = {10.1111/j.1467-9868.2005.00532.x}
}

@article{klosa:2020,
    author = {Klosa, J. and Simon, N. and Westermark, P.O. and Liebscher, V. and Wittenburg, D.},
    title = {Seagull: lasso, group lasso and sparse-group lasso regularization for linear regression models via proximal gradient descent},
    journal = {BMC Bioinformatics},
    volume = {21},
    number = {407}, 
    year = {2020},
    doi = {10.1186/s12859-020-03725-w}
}

@article{wright:2009,
  author={Wright, Stephen J. and Nowak, Robert D. and Figueiredo, MÁrio A. T.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Sparse Reconstruction by Separable Approximation}, 
  year={2009},
  volume={57},
  number={7},
  pages={2479-2493},
  doi={10.1109/TSP.2009.2016892}
}

@article{loris:2009,
    doi = {10.1088/0266-5611/25/3/035008},
    url = {https://dx.doi.org/10.1088/0266-5611/25/3/035008},
    year = {2009},
    month = {jan},
    publisher = {},
    volume = {25},
    number = {3},
    pages = {035008},
    author = {Ignace Loris},
    title = {On the performance of algorithms for the minimization of $\ell_1$-penalized functionals},
    journal = {Inverse Problems}
}

@book{brent:2013,
  title={Algorithms for minimization without derivatives},
  author={Brent, Richard P},
  year={2013},
  publisher={Courier Corporation}
}

@article{dekker:1969,
  title={Finding a zero by means of successive linear interpolation},
  author={Dekker, Theodorus Jozef},
  journal={Constructive aspects of the fundamental theorem of algebra},
  pages={37--51},
  year={1969},
  publisher={London}
}

@book{boyd:2004,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{simon:2012,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/24309971},
 abstract = {We re-examine the original Group Lasso paper of Yuan and Lin (2007). The form of penalty in that paper seems to be designed for problems with uncorrelated features, but the statistical community has adopted it for general problems with correlated features. We show that for this general situation, a Group Lasso with a different choice of penalty matrix is generally more effective. We give insight into this formulation and show that it is intimately related to the uniformly most powerful invariant test for inclusion of a group. We demonstrate the efficacy of this method–the "standardized Group Lasso"–over the usual group lasso on real and simulated data sets. We also extend this to the Ridged Group Lasso to provide within group regularization as needed. We discuss a simple algorithm based on group-wise coordinate descent to fit both this standardized Group Lasso and Ridged Group Lasso.},
 author = {Noah Simon and Robert Tibshirani},
 journal = {Statistica Sinica},
 number = {3},
 pages = {983--1001},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {STANDARDIZATION AND THE GROUP LASSO PENALTY},
 urldate = {2023-04-16},
 volume = {22},
 year = {2012}
}

@article{wang:2015,
  author  = {Jie Wang and Peter Wonka and Jieping Ye},
  title   = {Lasso Screening Rules via Dual Polytope Projection},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {32},
  pages   = {1063--1101},
  url     = {http://jmlr.org/papers/v16/wang15a.html}
}

@article{zou:2005,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3647580},
 abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
 author = {Hui Zou and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {301--320},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regularization and Variable Selection via the Elastic Net},
 urldate = {2023-04-16},
 volume = {67},
 year = {2005}
}
