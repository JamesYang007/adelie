\section{Group Lasso Problem}\label{sec:glp}

The lasso problem solves the following optimization problem:
\begin{align*}
    \minimize_{\beta \in \R^p}
    f(\beta)
    + \lambda \norm{\beta}_1
\end{align*}
where $f : \R^p \to \R$ is a convex function and $\lambda > 0$.
Given data $X \in \R^{n\times p}$ and $y\in \R^{n}$,
the most popular choices for $f$ are the Generalized Linear Model (GLM) 
negative log-likelihoods under the Gaussian and Binomial family:
\begin{align}
    &\text{Gaussian:} \quad f(\beta) := \frac{1}{2} \norm{y - X\beta}_2^2 
    \label{eq:glp:gaussian}\\
    &\text{Binomial:} \quad f(\beta) := \sum\limits_{i=1}^n \pr{y_i x_i^\top \beta - \log\pr{1 + e^{x_i^\top \beta}}}
    \label{eq:glp:binomial}
\end{align}
The celebrated lasso problem in regression settings as in the above 
gained wide popularity for its interpretability
as it shrinks some coefficients exactly to zero.\todojames{add more}

The group lasso problem~\citep{yuan:2006} is an extension of the lasso problem
that attempts to capture group structure within the features.
Specifically, the group lasso attempts to zero-out \emph{groups of coefficients} rather than a single coefficient.
The interpretation is that if a group of coefficients is zero, 
the whole group is uninformative in predicting $y$~\citep{meier:2008}.\todojames{Cite early literature of group lasso.}
This overcomes a major fallback of lasso with categorical features since
lasso selects individual categorical feature (factor)
instead of a whole group of features (e.g. one-hot encoded factors).
Moreover, the result of lasso changes based on the encoding of factors, 
which is an undesirable property.
\todojames{Cite GWAS and LD stuff that it makes a lot of sense to group genes.}

In preparation of posing the group lasso problem, we begin with some notation.
Let $1 \leq G \leq p$ be the number of groups among the $p$ features.
Let $p_i$ be the group size of group $i$ for each $1\leq i \leq G$
such that $\sum\limits_{i=1}^G p_i = p$.
Define $\beta = \br{\beta_1 \, \beta_2 \, \ldots \, \beta_G}^\top \in \R^p$
where each $\beta_i \in \R^{p_i}$.
The group lasso problem is to minimize the following objective:
\begin{align}
    \minimize_{\beta \in \R^p}
    f(\beta)
    + \lambda \sum\limits_{j=1}^G \norm{\beta_j}_2
    \label{eq:glp:objective}
\end{align}
for some convex function $f:\R^p \to \R$ and $\lambda > 0$.
Similar to the lasso, given data $X$ and $y$, we may use the same Gaussian and Binomial
log-likelihoods~(\ref{eq:glp:gaussian}),~(\ref{eq:glp:binomial}), respectively, for $f$.
